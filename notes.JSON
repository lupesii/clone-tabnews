{
  "Versão_Node.JS": [
    "Com o arquivo .nvmrc, basta executar 'nvm install' para baixar e instalar a versão desejada do Node.js."
  ],

  "Manifesto": [
    "code: npm init - Para iniciar um novo projeto Node.js e criar um arquivo package.json.",
    "code: npm install - Para instalar as dependências listadas no arquivo package.json."
  ],

  "Next Dev": [
    "Para tornar a visibilidade do servidor, vá até a opção PORTA no terminal.",
    "Caso a porta não seja criada com o comando, vá na antena localizada na parte inferior e crie a porta."
  ],

  "TAREFAS": [
    "PESQUISAR METODOLOGIAS PARA PROJETOS PESSOAIS",
    "PESQUISAR MELHORES FORMAS DE DESENVOLVER UMA MILESTONE/ISSUE DE INSEPTION"
  ],

  "CUIDADO COM OVERENGIEERING": [
    "Fazer coisas simples e torna-las complexa é mais facil e menos custoso do que o contrário"
  ],

  "PoC x MVC": [
    "PoC: Testes iniciais, com o menor custo possível, feitos para decidir qual caminho o projeto seguirá.",
    "MVP: Produto minimamente funcional para atender e resolver um problema, de acordo com a decisão tomada a partir dos testes (PoCs)."
  ],

  "Teste Automatizados": [
    "O que é test Runner? É simplismente um código que possui a capacidade de rodar outros códigos",
    "Como instalar: npm instal --save-dev jest",
    "Como criar um arquivo de test: O arquivo deve ser nomeado com nome.test.js, baseado na nossa organização de pastas ele estará dentro da pasta `test`",
    "O que deve conter: Cada teste é definido dessa forma - test('nome do teste', () => {   expect('Aqui fica o q será testado').toBe('E aqui o que deve ser')})",
    "TDD significa Desenvolvimento Orientado a Testes ou melhor, por Testes ou a Testes, Basicamente criamos nossos testes para depois criar nossas soluções para que o teste passe",
    "Podemos usar um regex, um filtro, para rodar teste somente, por exemplo, de uma pasta"
  ],

  "Tipos de Testes": [
    "Testes de Unidade → Os testes de unidade são testes que abordam e agem em cima de unidades do seu sistema, ele garante que pequenas partes de sua aplicação funcionem corretamente ",
    "Testes de Integração → Esses testes abordam partes um pouco mais complexas, são testes que garantem que combinações de unidades se integrem corretamente",
    "Testes E2E → São testes do inicio ao fim, até mesmo a parte do front-end, são testes mais pesados e demorados"
  ],

  "API (Application Programming Interface)": [
    "API é basicamente uma forma de sistemas se comunicarem, é uma interface de comunicação programática, isso quer dizer que ela contém apenas o file das informações, sem toda parte gráfica para o usuário",
    "Para testar a requisição de uma API no protocolo HTTP, use 'curl http://localhost:3000/api/status -v'"
  ],

  "Virtual Host": [
    "Um servidor autorizativo pode ter vários Hosts em sua caixinha, isso quer dizer que podemos acessar diversos sites de um mesmo servidor, basta explicarmos através do Cabeçalho qual Host queremos",
    "Teste com somento o IP do servidor: curl https://76.76.21.21 --insecure --verbose",
    "Note que o cabeçalho da nossa requsisção está indicando como destino final o HOST, quando a resposta do servidor é mandada é atribuido no cabeçalho o status(308), indicando um redicionamento para o IP definido em LOCATION",
    "Adicionando --header ‘Host: fintab.com.br’ na requisição, indicamos para o servidor qual host realmente desejamos, então assim, nos é retornado a página."
  ],

  "Versioning": [
    "URI Path Versioning: Nessa forma o **client** informa pela **URL** qual versão da **API** vai utilizar, por exemplo: **http://localhost:3000/api/v1/status**, **http://localhost:3000/api/v2/status**",
    "Header Versioning: Nessa forma o Client envia pelo cabeçalho da request qual versão da API irá utilizar "
  ],

  "Docker?": [
    "O Docker surgiu para resolver de forma prática e simples um problema recorrente na parte de testes locais, onde cada diferença em cada maquina poderia torna a execução da aplicação diferente, antes dele utiliza diversas VMs(Virtual Machine) para cada área do sistema, porém, dessa forma, se utilizava muito poder operacional, visto que, é necessário criar um SO(Sistema Operacional novo para cada VM. Com o Docker não é necessário isso, podemos criar um 'Container em nosso própria maquina que isola diversos processo e separa os recursos, sem a necessidade da criação de outro SO",
    "Na prática o docker processa seu Container baseado na imagem passada a ele",
    "É utilizado o Docker HUb como repositório de imagens",
    "As configurações do container são definidas no arquvio 'compose.yaml' que deve estar na pasta RAIZ, quando configurado basta digitarmos 'docker compose up'",
    "Como saber o status do meu container, use o comando docker ps -a, onde o -a significa todos, incluindo containers que não estejam sendo executados",
    "podemos colocar o -d para usar o docker em segundo plano",
    "Mas e se eu quiser reiniciar meu container? Usamos o comando docker compose -f infra/compose.yaml up -d --force-recreate"
  ],

  "Client PSQL": [
    "Para baixarmos utilizaremos o comando: sudo apt install postgresql-client",
    "Para conectarmos ao nosso banco usaremos: psql --host:localhost --username=postgres --port=5432",
    "Para sairmos usamos *barraInvertida*q"
  ],

  "PG": [
    "Para instalar: npm install pg",
    "Vamos utilizar o plugin do postgres para conseguir nos comunicamos com o banco de dados, pois esse plugin consegue 'conversar' no mesmo protocolo do banco",
    "Primeiramente criamos o módulo do `database.js` e exportamos o objeto query que executa a função query. Nesse módulo importamos o client do pg e criamos a função query, que dentro de mesma possuie, o objeto client que recebe todos as credenciais para se conectar ao banco, abrimos a conexão executamos a query e fechamos a conexão, sempre colocanco AWAIT para que execução não atropele as outras"
  ],

  "Variaveis de Ambiente - .env": [
    "Variaveis de ambiente são informações que o ambiente passa de forma dinâmica para o sistema, sem a necessidade de um hadcoded",
    "Para o `database.js` utilizar variavies de ambiente, temos que trocar os hardcoded e adicionar `process.env.nomeVariavel`",
    "Podemos passar as pelo prompt como por exemplo POSTGRES_PASSWORD=LOCAL_PASSWORD, LEMBRE-SE DE COLOCAR UM ESPAÇO ANTES DO COMANDO PARA NÃO SALVAR DADOS SENSIVEIS NO HISTORICO",
    "Porém existe um jeito mais fácil, o next vem com um módulo instalado, o dotenv, com ele você pode declarar as variaveis de ambiente em um arquivo .env na raiz do projeto",
    "O arquivo dotenv é apenas um arquivo de texto comum, não possui nenhuma lógica nele, então caso necessário uma DATABASE_URL utilizando interpolação é preciso baixar o módulo dotenv-expend que EXPANDE as opções dos arquivos dotenv"
  ],

  "Migrations - Linha de Comando": [
    "Nos tempos das pedras, os desenvolvedores, enquanto desenvolviam a aplicação criavam e motificavam o banco de dados, porém essas modificações eram locais, havendo a possibilidade de quando for passada para a produção, alguma alteração feito no ambiente de desenvolvimento não executar ou ser esquecida. Com esse objetivo foram criadas as migrations, que migram nosso banco de uma verso a outro de forma dinâmica",
    "Elas avaliam as mudanças ja executadas e as novas e executa somente as novas, não correndo o risco de que certas coisas fiquem inconsistentes",
    "Instalamos o módulo node-pg-migrate, também é necessario baixar o módulo dotenv para ler o arquivo .env.development",
    "Usamos o comando node-pg-migrate create *nome se quiser* para criar um arquivo de migração",
    "Na parte up, são todas as alterações que queremos fazer no banco",
    "Na parte down, são todas as alterações que queremos desfazer no banco, como fazer um drop"
  ],

  "Migrations - EndPoint": [
    "Para criarmos o EndPoint, precisamos primeiramente importar o migrationRunner, que irá rodar as migrações. Quando importado utilizamos o comando com await(já que é um comando assincrono) e indicamos a ele um objeto com diversas propriedades, como, a url do banco, o diretório das migrações, a direção que irá executar, etc",
    "Para os processos funcionarem de forma correta não podemos roda-los de forma paralela, já que cada test muda o state do banco, ficando complicado a execução dos mesmo, para fazermos isso, precisamos colocar no comando que executamos o parâmetro --runInBand",
    {
      "Usando Client para conectar ao modulo node-pg-migrate": [
        "Estamos fazendo isso para adicionar uma lógica de segurança a conexão do cliente, já que através da URL não é possivel utilizar uma lógica para, por exemplo, ssl ou certificados",
        "Para isso devemos desenvolver uma função no DATABASE.JS que cria para nós uma objeto client e exportamos essa função. Em seguida implementarmos no endpoint das migrations e lembre-se de fechar as conexões, pois como escrito no documento 'O fechamento do client fica por conta do usuário'"
      ]
    },
    {
      "Dry Run": [""]
    }
  ],

  "jest.config.js": [
    "O jest possui algumas limitações que podem nos atrapalhar ao decorrer do projeto, como: não transpilar os arquivos, não conseguir ler arquivos .env, não conseguir usar absolute imports",
    "Podemos resolver essas inconsistências usando o arquivo de configuração do jest, jest.config.js",
    "Nele, primeiramente, precisamos importar o modulo next/jest para o next dar 'poderes' ao jest. Feito isso ahora precisamos executar a função factory desse modulo nextJest() e guarda-la em uma constante(já que precisamos guardar a nova função gerada para executarmos). Depois precisamos executar essa função, passando junto um objeto com as configurações, e guardar em outra constante, para que no final possamos exportar.",
    {
      "configurações do objeto": [
        "moduleDirectories: ['node_modules', '<rootDir>']"
      ]
    }
  ],

  "join from node:path": [
    "Usamos esse módulo para evitamos o caminho errado em diferentes ambientes operacionais, por exemplo, o Linux usar 'infra/migrations', já o windows usa a barra invertida, causando assim um erro no ambiente Windows. O que esse comando faz é juntar e formar os caminhos, sem ocorrer esse problema",
    "join('infra', 'migrations')"
  ],

  "Branch": [
    {
      "Nivel 1": [
        "A branchs do git são como copias do projeto, onde em cada cópia é possivel fazer alterações diferentes, sem que uma afete a outro necessáriamente",
        "Importante destacas que essa explicação não é a melhor"
      ],

      "Nivel 2": [
        "Caso o git fizesse realmente um cópia do projeto, projetos com grandes volumes de dados demandariam tempo para trocar e criar branchs novas, e não é isso que acontece, eles são quase instântaneas",
        "Então como é feio, pode se entender que o git apenas recria um novo histórico, e nesse histórico os commits apontam para os seus respectivos blobs",
        "Porém ainda não é a correta"
      ],

      "Nivel 3": [
        "O git não clona nada, o que ele faz é mudar o apontamento, as branchs são como 'apelidos' para commits, pois ela referenciam commits",
        "Então quando mudamos de branch, apenas redirecionamos o apontamento do HEAD, que aponta para a branch, que por sua vez aponta para o commit. Assim nada é clonado, nem o histórico e nem a branch"
      ],

      "git branch -d": [
        "Quando deletamos uma branch, não deletamos os commits incluidos na mesma, apenas tiramos o ponteiro que referenciava ao ultimo commit dessa branch, logo usando a lógica o necessário para recuperar uma branch perdida é apenas encontrar novamente o ultimo commit dela(normalmente aparece no terminal), fazer um checkout para ele e criar uma branch, ou melhor um ponteiro, assim nada será perdido",
        "Caso você tenha perdido o hash do commit podemos usar o comando git reflog que abre o log de todas as referencias e por ele procurar o hash do commit"
      ],

      "Merge": [
        "O merge de forma simples, apenas move o ponteiro da branch target para o mesmo commit da branch source.",
        "Para fazermos o merge devemos primeiro entrar na branch que queremos atualizar (target) colocarmos o comando 'git merge branch-source', onde branch-source vai ser a branch que queremos como base. Caso tudo ocorra bem, fast-foward, não aparecerá nem um problema"
      ]
    }
  ],

  "Estratégias de branch": [
    {
      "Trunk-Based Development": [
        "Nessa estratégia todos os commits realizados são continuamente integrados com a branch trunk, ou melhor tronco, no nosso caso a main. Isso de inicio parece perigoso, pois diversos programadores integrando diferentes features direto na branch de produção, parece que vai explodir, por isso são adotadas algumas medidas para impedir isso",
        "FEATURE FLAG - Nessa medida, as features tem flags, marcadores, que sinalizam ao sistema o que ele deve exibir para cada usuário, um exemplo simples seria, features com flags de 'beta tester' serão exibidas para quem é 'beta tester' e não para o restante",
        "Além disso é muito importante sempre estar integrando constantemente o trunk, para diminuir a entropia entre os desenvolvedores, fazendo até mesmo commits parciais",
        "BRANCH BY ABSTRACTION -  Nessa medida são utilizadas abstrações para realizar grandes mudanças no sistema, por exemplo, os desenvolvedores utilizam um modulo A que irá ser mudado, então é adicionado uma abstração que liga ao módulo, e cada feature irá se ligar a essa abstração e quando todos estiverem integrados com a ela, a mesma poderá trocar o modulo na qual ela se liga, fazendo com que essa alteração não cause tanto prejuizo"
      ],

      "Feture Branch": [
        "Na branch main, serão commitados somente alterações prontas para o deploy, enquanto as features são desenvolvidas em outras branchs que posteriormente são integradas a main. Enquanto cada feature está sendo desenvolvida, o projeto estará também avançando, sendo então, importantissimo a correção de conflitos ao integrar a main",
        "O github envolve essa estratégia com ferramentas exelentes de visualização e revisão de código, sendo até chamada de github flow"
      ],

      "Git Flow": [
        "Essa estratégia é focada para sistemas que mantém multiplas versões do mesmo software. Sendo bem complexa",
        "Na main serão commitadas apenas as verções, a programação é desenvolvida na branch develop que também se ramifica para outras branchs features. Importante destacar que caso seja encontrada um bug critico em produção é criada a partir da main uma branch chamada de hotfix que conserta o bug e posteriormente é integrada na main e na branch develop.",
        "Quando uma feature é completada ela é revisada e integrada a branch develop, quando tudo nessa branch for completado, é passado para uma branch chamada de release, onde as modificações são revisadas minuciosamente, onde se for encontrado um bug é concertado direto na mesma branch e quando tudo estiver completo, integramos com a branch main, criando uma nova versão do software",
        "Sempre importante manter as branchs develop e feature no mesmo 'nivel' da main"
      ]
    }
  ],

  "Estabilizar npm run dev": [
    "Queremos que todas as dependencias do nosso sistema sejam diretamente acionadas apenas ao executar um comando, e por enquanto isso não ocorre, pois o sistema de migrations deve ser rodado separadamente. Colocando ele no mesmo comando do run dev, é visto um problema, depois de executar o up dos serviçoes postgres, que estam rodando no -d, passam direto para o comando das migrations, causando um problema de race condition, onde as migrations são executadas antes mesmo que o postgres consiga fazer conexões",
    "Então criamos o script 'wait-for-postgres' que verifica o estado do postgres até que ele retorne estar pronto.",
    "Nesse script importamos o exec do node:child_process, uma função que cria um novo processo de shell e executa um comando nele, que usaremos para executar um comando no qual o docker retornará a condição do postgres, docker exec postgres-dev pg_isready --host localhost, quando executado ele chama uma função de callback que procura na resposta do comando 'accepting connections', caso não exista ele executa novamente a verificação até que retorne a resposta esperada, siginificando assim, que o postgres está pronto para conexões"
  ],

  "Estabilizar npm run test": [
    "Agora temos outro problema, quando executamos o comando de teste sozinho, ele retorna erro em todos os testes, isso acontece pois os outros serviços não estão executando juntos, causando assim o erro.",
    "Primeiramente podemos começar adicionando ao comando a parte que levanta o docker e espera ele estar pronto, npm run services:up && npm run wait-for-postgres",
    "Ainda continua dando erro, isso porque o servidor não está sendo executado junto, o problema é: o servidor não roda em modo detached",
    "Usaremos então o concurrently para executar comando concorrentes, concurrently -n next,jest --hide next -k -s command-jest 'next dev' 'jest --runInBand'",
    {
      "Parâmetros": [
        "-n, abreviação de --names, nomeia o log de cada processo.",
        "--hide, esconde os logos de determinado processo",
        "-k, abreviação de --kill-others, mata todos os outros processo quando um retorna sucesso",
        "-s, abreviação de --sucess, retorna 0 quando um processo desejado retorne 0 também"
      ]
    },
    "Porém ainda existe a possibilidade de que acontece uma race condition, então utilizaremos o módulo async-retry e um orchestrator para evitar isso.",
    "Primeiramente devemos criar uma função no orchestrator para esperar um json vindo do endpoint /status, e utilizamos o async-retry para refazer essa função diversas vezes.",
    "Feito isso, devemos importar o orchestrator nos testes e faze-lo rodar antes de todos, junto com o 'clearDatabase'.Além de aumentarmos o timeout do jest e a quantidade de tentativas do async-retry"
  ],

  "CI - Continuous Integration": [
    "Agora começaremos a desenolver o CI do nosso sistema, vamos utilizar o github actions para que tudo ocorra bem. O github actions é um funcionalidade do github que pode executar diversas actions(tanto do marketplace quanto programadas por você mesmo) com gatilhos, por exemplo um pull request",
    "Para a configuração do github actions precisamos criar uma pasta '.github' na raiz do projeto e dentro dela outra pasta 'workflows' e por fim dentro dessa o arquivo do actions, tests.yaml",
    {
      "Estrutura do arquivo": [],
      "Workflow": "Testes Automatizados",
      "Events": "Pull Request",
      "Job": "Jest",
      "Runner": "Ubuntu",
      "Step": "Instalar dependencias",
      "step": "Rodar baterias"
    },
    "Dentro do arquivo tests.yaml, vamos usar o sistema Ubuntu em sua ultima versão. Além disso, serão 4 passos: ",
    "actions/checkout, para clonar nosso projeto dentro da maquina virtual do workflow",
    "actions/setup-node, para definirmos a versão do nodejs",
    "npm ci, que irá baixar as dependências do package-lock, ou seja, nas mesmas versões utilizadas no desenvolvimento do projeto",
    "npm test",
    "Agora que todos os testes automatizados conseguem rodar antes de um merge, caso eles falhem o botão de merge deve ser desabilitado, e isso deve ser feito nas rules de branch"
  ]
}
